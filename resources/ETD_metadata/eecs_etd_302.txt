$TITLE:
Correlated Sources In Distributed Networks - Data Transmission, Common Information Characterization and Inferencing

$DEPARTMENT:
Electrical Engineering and Computer Science

$KEYWORDS:
Common Information, Correlated Sources, Interference Channels, Joint Source Channel Coding, Test of Independence

$ABSTRACT:
Correlation is often present among observations in a distributed  system. This thesis deals with various design issues when correlated data are  observed at distributed terminals, including: communicating correlated sources  over interference channels, characterizing the common information among  dependent random variables, and testing the presence of dependence among  observations.It is well known that separated source and channel coding is  optimal for point-to-point communication. However, this is not the case for  multi-terminal communications. In this thesis, we study the problem of  communicating correlated sources over interference channels (IC), for both the  lossless and the lossy case. For lossless case, a sufficient condition is found  using the technique of random source partition and correlation preserving  codeword generation. The sufficient condition reduces to the Han-Kobayashi  achievable rate region for IC with independent observations. Moreover, the  proposed coding scheme is optimal for transmitting a special correlated sources  over a class of deterministic interference channels. We then study the general  case of lossy transmission of two correlated sources over a two-user discrete  memoryless interference channel (DMIC). An achievable distortion region is  obtained and Gaussian examples are studied.The second topic is the generalization of Wyner's definition of  common information of a pair of random variables to that of N random variables.  Coding theorems are obtained to show that the same operational meanings for the  common information of two random variables apply to that of N random variables. We  establish a monotone property of Wyner's common information which is in contrast  to other notions of the common information, specifically Shannon's mutual  information and G'{a}cs and K"{o}rner's common randomness. Later, we extend  Wyner's common information to that of continuous random variables and provide an  operational meaning using the Gray-Wyner network with lossy source coding. We  show that Wyner's common information equals the smallest common message rate  when the total rate is arbitrarily close to the rate-distortion function  with joint decoding.Finally, we consider the problem of distributed test of  statistical independence under communication constraints. Focusing on the  Gaussian case because of its tractability, we study in this thesis the  characteristics of optimal scalar quantizers for distributed test of independence where the optimality is both in the finite sample  regime and in the asymptotic regime.