computer science electrical engineering this thesis presents research into parallel linear solvers for block-diagonal-bordered sparse matrices the block-diagonal-bordered form identifies parallelism that can be exploited for both direct and iterative linear solvers we have developed efficient parallel block-diagonal-bordered sparse direct methods based on both lu factorization and choleski factorization algorithms and we have also developed a parallel block-diagonal-bordered sparse iterative method based on the gauss-seidel method parallel factorization algorithms for block-diagonal-bordered form matrices require a specialized ordering step coupled to an explicit load balancing step in order to generate this matrix form and to distribute the computational workload uniformly for an irregular matrix throughout a distributed-memory multi-processor matrix orderings are performed using a diakoptic technique based on node-tearing-nodal analysis parallel gauss-seidel algorithms for block-diagonal-bordered form matrices require a two-part matrix ordering technique--first to partition the matrix into block-diagonal-bordered form again using the node-tearing diakoptic techniques and then to multi-color the data in the last diagonal block using graph coloring techniques the ordered matrices have extensive parallelism while maintaining the strict precedence relationships in the gauss-seidel algorithmempirical performance measurements for real power system networks are presented for implementations of a parallel block-diagonal-bordered lu algorithm a similar choleski algorithm and a parallel block-diagonal-bordered gauss-seidel algorithm run on a distributed memory thinking machines cm-5 multi-processor we have compared the performance of the direct and iterative parallel implementations on the cm-5 and show that significant algorithmic speedup may be possible for the gauss-seidel algorithm versus choleski factorization for positive definite matrices we have developed a simple technique that uses empirical data to predict the performance of these algorithms on future architectures we apply these techniques to develop algorithm performance predictions for future scalable parallel processing spp architectures