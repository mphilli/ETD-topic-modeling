common information correlated sources interference channels joint source channel coding test of independence correlation is often present among observations in a distributed  system this thesis deals with various design issues when correlated data are  observed at distributed terminals including communicating correlated sources  over interference channels characterizing the common information among  dependent random variables and testing the presence of dependence among  observationsit is well known that separated source and channel coding is  optimal for point-to-point communication however this is not the case for  multi-terminal communications in this thesis we study the problem of  communicating correlated sources over interference channels ic for both the  lossless and the lossy case for lossless case a sufficient condition is found  using the technique of random source partition and correlation preserving  codeword generation the sufficient condition reduces to the han-kobayashi  achievable rate region for ic with independent observations moreover the  proposed coding scheme is optimal for transmitting a special correlated sources  over a class of deterministic interference channels we then study the general  case of lossy transmission of two correlated sources over a two-user discrete  memoryless interference channel dmic an achievable distortion region is  obtained and gaussian examples are studiedthe second topic is the generalization of wyner's definition of  common information of a pair of random variables to that of n random variables  coding theorems are obtained to show that the same operational meanings for the  common information of two random variables apply to that of n random variables we  establish a monotone property of wyner's common information which is in contrast  to other notions of the common information specifically shannon's mutual  information and g'acs and korner's common randomness later we extend  wyner's common information to that of continuous random variables and provide an  operational meaning using the gray-wyner network with lossy source coding we  show that wyner's common information equals the smallest common message rate  when the total rate is arbitrarily close to the rate-distortion function  with joint decodingfinally we consider the problem of distributed test of  statistical independence under communication constraints focusing on the  gaussian case because of its tractability we study in this thesis the  characteristics of optimal scalar quantizers for distributed test of independence where the optimality is both in the finite sample  regime and in the asymptotic regime