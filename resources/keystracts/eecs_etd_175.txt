parallel processing collective data transfer distributed computing advances in computing and networking infrastructure have enabled an increasing number of applications to utilize resources that are geographically distributed high-performance distributed computing environments present significant new problems and opportunities for programming language designers compiler implementors and runtime system developers this thesis addresses several issues in providing efficient mechanisms for collective data transfer among sets of processes in distributed environments in particular it examines how such mechanisms are employed in two contexts to achieve high performance in application programs the techniques are designed to provide efficiency portability and ease of programmingmany scientific applications need to access data resources from multiple locations in a distributed computing environment this thesis presents the design and implementation of a system called rio that provides remote io functionality programs use familiar parallel io interfaces to access remote file systems the rio library offers significant performance flexibility and convenience advantages over existing techniques for remote data access such as manual staging and deployment of distributed file systems mechanisms provided by the globus metacomputing toolkit are used to support the integration of the library in a distributed environment experiments using synthetic benchmarks and real applications that use rio demonstrate performance benefits over staging techniques and current distributed file systemsprogramming models for distributed systems will need to exploit all types of parallelism within application domains by utilizing a mixture of both task and data parallelism in parallel applications one may extend the range of problems that can be solved efficiently beyond what is possible with pure data-parallel programming languages such as high performance fortran alone this thesis proposes a programming model where common parallel program structures can be represented with only minor extensions to the hpf model by using a coordination library based on the message passing interface mpi this library allows data-parallel tasks to exchange distributed data structures using calls to simple communication functions experiments with microbenchmarks and applications characterize the performance of this library and quantify the impact of various optimizations these results demonstrate that the hpfmpi library can provide performance superior to that of pure hpf