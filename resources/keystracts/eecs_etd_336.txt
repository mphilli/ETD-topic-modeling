common information data reduction decentralized inference dependent observations sufficiency statistics wyner's common information was originally defined for a pair of dependent discrete random variables this thesis generalizes its definition in two directions the number of dependent variables can be arbitrary so are the alphabets of those random variables new properties are determined for the generalized wyner's common information of multiple dependent variables more importantly a lossy source coding interpretation of wyner's common information is developed using the gray-wyner network it is established that the common information equals to the smallest common message rate when the total rate is arbitrarily close to the rate distortion function with joint decoding if the distortions are within some distortion regionthe application of wyner's common information to inference problems is also explored in the thesis a central question is under what conditions does wyner's common information capture the entire information about the inference object under a simple bayesian model it is established that for infinitely exchangeable random variables that the common information is asymptotically equal to the information of the inference object for finite exchangeable random variables connection between common information and inference performance metrics are also establishedthe problem of decentralized inference is generally intractable with conditional dependent observations a promising approach for this problem is to utilize a hierarchical conditional independence model utilizing the hierarchical conditional independence model we identify a more general condition under which the distributed detection problem becomes tractable thereby broadening the classes of distributed detection problems with dependent observations that can be readily solvedwe then develop the sufficiency principle for data reduction for decentralized inference for parallel networks the hierarchical conditional independence model is used to obtain conditions such that local sufficiency implies global sufficiency for tandem networks the notion of conditional sufficiency is introduced and the related theory and tools are developed connections between the sufficiency principle and distributed source coding problems are also explored furthermore we examine the impact of quantization on decentralized data reduction the conditions under which sufficiency based data reduction with quantization constraints is optimal are identified they include the case when the data at decentralized nodes are conditionally independent as well as a class of problems with conditionally dependent observations that admit conditional independence structure through the hierarchical conditional independence model