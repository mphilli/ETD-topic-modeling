computer science in recent years distributed memory parallel machines have been widely recognized as the most likely means of achieving teraflops performance however programming a distributed memory machine to get good speed-ups and efficiency proves to be cumbersome to ease the task of programming parallel machines recently there have been major efforts in developing programming language and compiler support for distributed memory machinesthere exists a class of scientific and engineering applications called irregular applications in which many of the optimizations can be done only at runtime this constraint presents a greater challenge for compilers this research provides solutions for compiling irregular problems this thesis presents a combined runtime and compile-time approach for parallelizing this general class of applications on distributed memory machines it presents a runtime system that has been designed and implemented for parallelizing these applications on distributed memory machines methods by which compilers for high performance fortran hpf style parallel programming languages can automatically generate calls to the runtime system are also presentedthe runtime system supports the partitioning of loop iterations to maintain data locality the coupling of data partitioners to obtain non-standard distribution the remapping of data structures and optimizations such as vectorization aggregation and schedule reuse the compiler techniques have been implemented in the fortran 90dhpf compiler being developed at syracuse university the runtime and compile-time approaches have been evaluated using templates from real scientific applications performance results of fortran 90d compiler-parallelized codes are compared with that of hand-parallelized codes it is observed that the compiler-generated codes perform within 15 of the hand-parallelized codes