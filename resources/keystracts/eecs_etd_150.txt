cache memory compiler many applications are memory intensive and thus are bounded by memory latency and bandwidth while improving memory performance is an important problem for uniprocessor machines it is even more important for shared memory multiprocessors such as cc-numa cache-coherent non-uniform memory access machines an architectural solution to the long memory latencies problem is to adopt a hierarchical memory system where the higher levels eg caches are expensive small and fast whereas the lower levels eg memories secondary and tertiary storages are cheaper larger and slower since the access latency varies from level to level it is very important to utilize this memory hierarchy fully by having the majority of data accesses satisfied from the higher levels ie caches this dissertation first proposes a data transformation framework to improve cache locality instead of modifying the execution order of loop iterations as done by most of the current compiler techniques the data transformations modify the memory layouts of large arrays we illustrate the cases where data memory layout transformations are preferable to loop transformations and show how to generate code after a data layout transformation later we present a unified data locality optimization framework where both loop and data transformations are used in concert to improve cache locality we demonstrate that such a unified framework achieves better results than frameworks based on pure loop or pure data transformations we also quantify the benefits by giving performance numbers obtained on a single processor as well as multiple processors of sgicray origin 2000 a cc-numa machine our performance results on a uniprocessor as well as multiprocessors show marked improvements in cache misses and overall execution times this unified technique can also be used for main memory-secondary storage hierarchy we also briefly discuss how our approach can be made to work in an inter-procedural setting