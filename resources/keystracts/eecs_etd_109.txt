neural networks evolutionary computation multiphase particle swarm optimization a large number of problems can be cast as optimization problems in which the objective is to find a set of values for problem parameters that maximize or minimize an objective fitness or cost function this work proposes the multi-phase particle swarm optimization  mppso  algorithm extending the particle swarm optimization  pso  algorithm recently proposed to solve optimization problems and allowing it to be applied to both continuous and discrete space optimization problems the pso algorithm falls into the evolutionary computation paradigm it does not need gradient information unlike most neural network and gradient descent algorithms it evolves a population of individuals called particles each particle moves around the search space updating its velocity and position based on the best positions thus far discovered by itself and by other particles the mppso algorithm extends pso utilizing multiple groups of particles with different goals that change with time alternately moving toward or away from the best of the candidate solutions discovered thus far it performs better and is less likely to be trapped in a local optimum than the pso strategy in which all particles move towards the best solution discovered thus far the mppso algorithm also enforces a steady improvement in solution quality accepting only moves that improve fitness this eliminates a considerable amount of wasted search effort that would be spent in relatively poor particle positions the possibility of remaining stuck in local optima is further reduced by periodically reinitializing particle velocities which is a more useful strategy than randomly reinitializing particle positions experimental simulations show that the proposed algorithm outperforms a genetic algorithm an evolutionary programming algorithm and a previous version of the particle swarm optimization algorithm on several difficult benchmark problems in both discrete and continuous spaces it managed to reach optimum fitness using fewer fitness evaluations and less computation time than the other algorithms another set of simulations showed that the algorithm is successful in training a two-layer feedforward neural network reaching lower error values than the backpropagation algorithm