backpropagation statistics mathematics computer science artificial intelligence in this study we focus on feed-forward neural networks with a single hidden layer the research touches upon several important issues in artificial neural networks such as the reliability and generalization of trained networks the convergence of the learning algorithm in the computational sense and the strong consistency of the stable states of networks in the statistical sense have been addressed as major measures of reliability and generalization respectively based on the internal structure of feed-forward neural networks with a single hidden layer two-stage learning is proposed to implement two-stage learning we proposed two new learning algorithms--two-stagels and two-stagegibbs the reliability and generalization of these two learning algorithms ie the convergence in the computational sense and the strong consistency in the statistical sense are rigorously studied these optimal properties of proposed learning algorithms are further confirmed by intensive empirical studies such as comparisons made on the fisher's iris data 1939 the use of multiple measurements in taxonomic problems ann eugenics 7 pt ii pp 197-188 between the proposed learning algorithms and statistical methods like bayesian discriminate analysis kernel density methods and k-nearest neighbors comparisons between the proposed learning algorithms and other existing learning algorithms like backpropagation and simulation studies both theoretical and empirical studies demonstrate the potential of the proposed algorithms to the real world